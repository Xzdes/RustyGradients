# Rusty Gradients

Rusty Gradients — это фундаментальный фреймворк для глубокого обучения на Rust, построенный вокруг динамического графа вычислений и автоматического дифференцирования (autograd).

Проект создан с целью предоставить ясный, надежный и расширяемый набор инструментов для создания и обучения нейронных сетей, уделяя особое внимание безопасности и контролю, которые предоставляет язык Rust.

## Ключевые особенности

*   **Динамический граф вычислений**: Легко создавайте сложные, изменяемые архитектуры моделей. `Tensor` автоматически отслеживает все операции для построения графа.
*   **Автоматическое дифференцирование**: Вызовите `.backward()` на скалярном тензоре (например, на результате функции потерь), чтобы автоматически рассчитать градиенты для всех обучаемых параметров.
*   **Богатая библиотека слоев**: Включает в себя все необходимые компоненты для современных архитектур: `Linear`, `ReLU`, `LayerNorm`, `Embedding`, `MultiHeadAttention`, `TransformerBlock` и другие.
*   **Современные оптимизаторы**: Встроенные реализации `SGD` и `Adam` для эффективного обучения моделей.
*   **Надежная обработка ошибок**: API, основанный на типе `Result`, позволяет элегантно обрабатывать потенциальные ошибки (например, несовпадение размерностей тензоров) без паники.

## Установка

Добавьте библиотеку в ваш `Cargo.toml`:

```toml
[dependencies]
rusty-gradients = "0.1.0" # Укажите актуальную версию с crates.io
```

## Быстрый старт: Обучение MLP на задаче XOR

Этот пример демонстрирует полный цикл обучения простой нейронной сети для решения классической нелинейной задачи XOR.

```rust
use rusty_gradients::nn::{Linear, Module, ReLU, Sequential};
use rusty_gradients::optim::{Adam, Optimizer};
use rusty_gradients::tensor::Tensor;
use rusty_gradients::losses::mse_loss;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // 1. Определяем данные для задачи XOR
    let training_data = Tensor::new(
        ndarray::array![[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]].into_dyn(),
        false,
    );
    let training_labels = Tensor::new(
        ndarray::array![[0.0], [1.0], [1.0], [0.0]].into_dyn(),
        false,
    );

    // 2. Создаем модель: 2 входа -> 4 скрытых нейрона -> 1 выход
    let model = Sequential::new(vec![
        Box::new(Linear::new(2, 4)),
        Box::new(ReLU::new()),
        Box::new(Linear::new(4, 1)),
    ]);

    // 3. Создаем оптимизатор Adam
    let mut optimizer = Adam::new(model.parameters(), 0.01, None, None);

    // 4. Запускаем цикл обучения
    for epoch in 0..=1000 {
        // Прямой проход
        let predictions = model.forward(&training_data)?;
        // Вычисление функции потерь
        let loss = mse_loss(&predictions, &training_labels);

        // Обратное распространение ошибки
        loss.backward();
        // Обновление весов
        optimizer.step();
        // Обнуление градиентов
        optimizer.zero_grad();

        if epoch % 100 == 0 {
            println!("Эпоха: {}, Потери: {:.4}", epoch, loss.data.borrow().sum());
        }
    }

    // 5. Проверяем результат после обучения
    let final_predictions = model.forward(&training_data)?;
    println!("\nРезультаты после обучения:");
    println!("{}", final_predictions.data.borrow());

    Ok(())
}
```
## План развития

1.  **Разделение `Tensor` на `Node` и `Value`**:
    *   Сейчас ваш `Tensor` смешивает две концепции: узел в графе вычислений (`ctx`, `grad`) и конкретное значение (`data`).
    *   В Synapse вам нужно будет разделить это. У вас будет `Asg` со списком `Node`. А отдельно будет `Runtime`, который во время выполнения будет сопоставлять `NodeID` с конкретными `Value` (числами, строками, структурами).
    *   Ваша текущая реализация `Tensor` станет **особым типом `Value`** в Runtime-среде Synapse, который поддерживает дифференцирование.

2.  **От `ops` к `NodeType`**:
    *   Ваши текущие `ops` (`dot_op`, `relu_op` и т.д.) — это конкретные реализации для `NodeType::BinaryOperation`, `NodeType::Application` и других.
    *   Вам нужно будет создать общий `enum NodeType`, как в вашей спецификации, и ваш интерпретатор будет делать `match` по этому `enum`, чтобы вызвать соответствующую реализацию (например, ваш `dot_op`).

3.  **От `backward()` к общему `evaluate()`**:
    *   Ваш `backward()` — это специализированный "интерпретатор", который вычисляет градиенты.
    *   Вам понадобится более общий интерпретатор `evaluate()`, который сможет не только считать градиенты, но и просто выполнять код — складывать числа, выводить строки, работать с файлами.
    *   `backward()` останется частью этого интерпретатора, которая будет вызываться для дифференцируемых подграфов.

4.  **Сериализация `SYN1`**:
    *   Это совершенно новый слой, которого сейчас нет, но вы его подробно описали. Вы можете создать его, отталкиваясь от структур Rusty Gradients. Вместо того чтобы сериализовывать `Tensor`, вы будете сериализовывать более общую структуру `Node`.

### Конкретный план действий: "Synapse на стероидах Rusty Gradients"

Я бы предложил такой план, который использует вашу текущую кодовую базу как трамплин:

**Этап 1: Обобщение ядра Rusty Gradients.**
1.  **Переименуйте проект в `synapse_core`**. Это будет ваше ядро.
2.  **Разделите `Tensor`**:
    *   Создайте `struct Node` и `struct Edge`, как в спецификации.
    *   Создайте `struct Asg { nodes: Vec<Node> }`.
    *   Создайте `enum Value { Int(i64), Float(f32), Tensor(Rc<RefCell<ArrayD<f32>>>), ... }`.
    *   Ваш текущий `Tensor` станет `struct DifferentiableTensor`, который будет жить внутри `Value::Tensor`.
3.  **Создайте `NodeType` и `EdgeType` `enum`'ы**.

**Этап 2: Построение интерпретатора.**
1.  **Напишите простейший `evaluate`**: Он будет брать `Asg`, `NodeID` и хэш-мапу `HashMap<NodeID, Value>` (контекст). Он будет выполнять только простейшие операции (сложение `Value::Int`, например).
2.  **Интегрируйте Rusty Gradients**: Когда `evaluate` встретит узел, соответствующий дифференцируемой операции (например, `dot`), он вызовет ваш существующий код из `ops`, который будет работать с `Value::Tensor` и создавать `DifferentiableTensor` с `ctx`.
3.  **Реализуйте `backward` как специальную функцию Runtime**: Эта функция будет брать `NodeID` конечного `DifferentiableTensor` и запускать ваш существующий алгоритм обхода графа.

**Этап 3: Сериализация и инструменты.**
1.  **Реализуйте кодер/декодер `SYN1`**, который будет сохранять и загружать ваш `Asg`.
2.  **Создайте CLI `synapse-run`**, который загружает `.syn1` файл, вызывает `evaluate` и печатает результат.


## Лицензия

Этот проект распространяется под [лицензией MIT](LICENSE).