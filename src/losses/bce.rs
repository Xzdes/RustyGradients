use crate::tensor::Tensor;
// --- ИСПРАВЛЕНИЕ: Удаляем `Sub` из импортов ---
use std::ops::{Add, Mul};

/// Вычисляет Бинарную Перекрестную Энтропию (Binary Cross-Entropy Loss).
///
/// Эта функция потерь идеально подходит для задач бинарной классификации,
/// когда выход модели (после сигмоиды) представляет собой вероятность.
///
/// Формула: `Loss = -mean(y * log(p) + (1 - y) * log(1 - p))`.
/// Мы, для простоты, будем использовать `sum` вместо `mean`.
///
/// # Аргументы
///
/// * `p` - Тензор с предсказанными вероятностями (выходы модели).
/// * `y` - Тензор с истинными метками (0.0 или 1.0).
///
/// # Возвращает
///
/// Скалярный `Tensor`, содержащий значение ошибки.
pub fn bce_loss(p: &Tensor, y: &Tensor) -> Tensor {
    // Создаем тензор из единиц, чтобы вычислить `1 - y` и `1 - p`.
    // Он не требует градиента.
    let ones = Tensor::ones(p.data.borrow().shape(), false);

    // Первое слагаемое: y * log(p)
    let term1 = y.mul(&p.log());

    // Второе слагаемое: (1 - y) * log(1 - p)
    let term2 = (ones.sub(y)).mul(&(ones.sub(p).log()));

    // Сумма слагаемых: y*log(p) + (1-y)*log(1-p)
    let sum_terms = term1.add(&term2);

    // Усредняем (суммируем) и инвертируем знак.
    // Добавляем небольшой минус перед результатом, чтобы получить -sum(...)
    let neg_one = Tensor::new(ndarray::arr0(-1.0).into_dyn(), false);
    let loss = sum_terms.sum().mul(&neg_one);
    
    loss
}