//! Модуль, определяющий основной трейт для всех оптимизаторов.

// --- ИЗМЕНЕНИЕ: Убираем `use`, так как он не нужен на уровне модуля ---

/// Трейт, определяющий общий интерфейс для всех оптимизаторов.
///
/// Оптимизатор отвечает за обновление обучаемых параметров (весов) модели
/// на основе вычисленных градиентов.
pub trait Optimizer {
    /// Выполняет один шаг оптимизации (обновляет параметры).
    ///
    /// Этот метод изменяет состояние оптимизатора (например, моменты в Adam)
    /// и данные в параметрах модели, поэтому он принимает `&mut self`.
    ///
    /// # Пример использования в цикле обучения
    /// rust,no_run
    /// # use rusty_gradients::nn::{Linear, Module};
    /// # use rusty_gradients::optim::{Optimizer, SGD};
    /// # use rusty_gradients::tensor::Tensor;
    /// #
    /// # let model = Linear::new(10, 1);
    /// # let mut optim = SGD::new(model.parameters(), 0.01);
    /// # // --- ИСПРАВЛЕНИЕ: Указываем корректную форму для тензора ---
    /// # let dummy_input = Tensor::zeros(&, false);
    /// # let loss = model.forward(&dummy_input).unwrap();
    /// // 1. Вычисляем градиенты
    /// loss.backward();
    /// // 2. Обновляем веса с помощью оптимизатора
    /// optim.step();
    /// // 3. Обнуляем градиенты для следующей итерации
    /// optim.zero_grad();
    fn step(&mut self);

    /// Обнуляет градиенты всех параметров, которые отслеживает оптимизатор.
    ///
    /// Это необходимо делать перед каждым вызовом `backward()`, чтобы градиенты
    /// не накапливались от итерации к итерации. Этот метод не изменяет
    /// внутреннее состояние оптимизатора, только градиенты в самих тензорах,
    /// поэтому достаточно `&self`.
    fn zero_grad(&self);
}